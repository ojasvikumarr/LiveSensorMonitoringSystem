version: '3.8'

services:
  redis:
    image: redis:7-alpine
    container_name: redis-sensor
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --maxmemory 200mb --maxmemory-policy volatile-lru --save 60 1000 --lazyfree-lazy-eviction yes
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
    networks:
      - sensor-network

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper-sensor
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
    networks:
      - sensor-network

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka-sensor
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true
      # Retention policies for data cleanup - keep only 1 hour of data
      KAFKA_LOG_RETENTION_MS: 3600000  # 1 hour
      KAFKA_LOG_RETENTION_BYTES: 268435456  # 256MB max per topic
      KAFKA_LOG_SEGMENT_BYTES: 33554432  # 32MB segments
      KAFKA_LOG_SEGMENT_MS: 1800000  # 30 minutes per segment
      KAFKA_LOG_CLEANUP_POLICY: delete
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_LOG_CLEANER_DELETE_RETENTION_MS: 3600000  # 1 hour
      KAFKA_AUTO_LEADER_REBALANCE_ENABLE: "true"
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - sensor-network

  producer-service:
    build:
      context: ./producer-service
      dockerfile: Dockerfile
    container_name: producer-service
    ports:
      - "8081:8081"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    depends_on:
      - kafka
      - redis
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - sensor-network

  consumer-service:
    build:
      context: ./consumer-service
      dockerfile: Dockerfile
    container_name: consumer-service
    ports:
      - "8082:8082"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - SPRING_DATA_REDIS_HOST=redis
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - JAVA_OPTS=-Xmx512m -Xms256m -XX:+UseG1GC -XX:MaxGCPauseMillis=200
    depends_on:
      - kafka
      - redis
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - sensor-network

  api-service:
    build:
      context: ./api-service
      dockerfile: Dockerfile
    container_name: api-service
    ports:
      - "8083:8083"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - SPRING_DATA_REDIS_HOST=redis
    depends_on:
      - redis
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - sensor-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: frontend
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - USE_VM_IP=true
    depends_on:
      - api-service
      - producer-service
      - consumer-service
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - sensor-network

volumes:
  redis_data:

networks:
  sensor-network:
    driver: bridge
